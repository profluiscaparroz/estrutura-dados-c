# Algoritmos de Consenso Distribu√≠do: Paxos e Raft

## Introdu√ß√£o

Em sistemas distribu√≠dos, garantir que m√∫ltiplos processos concordem sobre um valor comum ‚Äî mesmo na presen√ßa de falhas ‚Äî √© fundamental para manter a **consist√™ncia** e a **disponibilidade** dos servi√ßos. Esse problema √© conhecido como **consenso distribu√≠do**.

Dois dos algoritmos mais importantes para resolver o consenso distribu√≠do s√£o o **Paxos**, desenvolvido por Leslie Lamport, e o **Raft**, criado por Diego Ongaro e John Ousterhout. Neste texto, exploraremos a origem, o funcionamento, as vantagens e desvantagens desses algoritmos, e seu impacto na computa√ß√£o distribu√≠da.

---

## Hist√≥ria e Contexto

### Antes do Paxos

Antes do Paxos, os sistemas distribu√≠dos enfrentavam o desafio de coordenar m√∫ltiplas m√°quinas para manter uma vis√£o consistente dos dados, principalmente em ambientes com falhas de comunica√ß√£o ou de m√°quinas.

O problema do consenso foi formalizado na d√©cada de 1980, com a publica√ß√£o do famoso trabalho de Fischer, Lynch e Paterson (FLP) em 1985, que provou que √© imposs√≠vel atingir consenso de forma determin√≠stica em sistemas ass√≠ncronos se mesmo um n√≥ pode falhar (FLP Impossibility Result). Isso fez com que a comunidade se voltasse para solu√ß√µes que pudessem garantir consenso em ambientes parcialmente s√≠ncronos ou com probabilidades de falha.

### Paxos (Leslie Lamport, 1998)

Em 1998, Leslie Lamport publicou o artigo ‚ÄúThe Part-Time Parliament‚Äù \[Lamport, 1998], onde apresentou o algoritmo **Paxos**, um m√©todo formal para alcan√ßar consenso em sistemas distribu√≠dos com falhas.

O nome ‚ÄúPaxos‚Äù vem de uma ilha grega, escolhida arbitrariamente por Lamport. O algoritmo foi inicialmente visto como complexo e dif√≠cil de entender, mas se tornou a base para muitos sistemas distribu√≠dos modernos.

Paxos resolve o problema do consenso mesmo que alguns dos n√≥s falhem ou que as mensagens sejam atrasadas, garantindo seguran√ßa (n√£o h√° dois processos que decidem valores diferentes) e eventual progresso (desde que algumas condi√ß√µes sejam atendidas).

### Raft (Ongaro & Ousterhout, 2014)

Em 2014, Diego Ongaro e John Ousterhout publicaram o artigo ‚ÄúIn Search of an Understandable Consensus Algorithm‚Äù \[Ongaro & Ousterhout, 2014]. Eles destacaram que, apesar da robustez do Paxos, sua complexidade dificulta a implementa√ß√£o e o ensino do algoritmo.

Raft foi criado com o objetivo de ser um algoritmo de consenso que mant√©m as mesmas garantias de Paxos, mas com design modular, mais f√°cil de entender, implementar e depurar. Raft foca em decompor o problema em subproblemas, como elei√ß√£o de l√≠der, replica√ß√£o de log e seguran√ßa.

---

## Por que esses algoritmos surgiram?

O consenso distribu√≠do √© um problema crucial porque, em sistemas distribu√≠dos (como bancos de dados replicados, sistemas de arquivos distribu√≠dos, ou plataformas de microsservi√ßos), diferentes r√©plicas devem concordar em qual opera√ß√£o aplicar e em qual ordem.

Antes de Paxos e Raft, as solu√ß√µes baseavam-se em:

* **Protocolos de coordena√ß√£o centralizada:** Que dependiam de um servidor mestre √∫nico (single point of failure).
* **Solu√ß√µes baseadas em bloqueios distribu√≠dos:** Que podiam causar deadlocks ou perda de disponibilidade.
* **Algoritmos ad hoc**, muitas vezes dif√≠ceis de provar corretos.

Paxos trouxe uma solu√ß√£o formal e tolerante a falhas. Raft melhorou a aplicabilidade pr√°tica e a ado√ß√£o industrial.

---

## Funcionamento b√°sico

### Paxos

Paxos √© baseado em tr√™s pap√©is principais: **proposers** (que prop√µem valores), **acceptors** (que aceitam propostas) e **learners** (que aprendem o valor decidido).

O protocolo ocorre em duas fases:

1. **Fase Prepare (prepare phase):** O proposer envia uma proposta com n√∫mero √∫nico para os acceptors pedindo promessa para n√£o aceitar propostas antigas.
2. **Fase Accept (accept phase):** Se os acceptors prometeram, o proposer envia a proposta para aceita√ß√£o.

O consenso √© alcan√ßado quando a maioria dos acceptors aceita o mesmo valor.

### Raft

Raft divide o consenso em tr√™s componentes principais:

* **Elei√ß√£o de l√≠der:** Os servidores elegem um l√≠der respons√°vel por gerenciar a replica√ß√£o.
* **Replica√ß√£o de log:** O l√≠der aceita comandos do cliente, os adiciona ao seu log e replica para os seguidores.
* **Seguran√ßa:** Garantir que logs dos servidores converjam e que as decis√µes sejam consistentes.

Raft assume que o l√≠der √© o ponto central de coordena√ß√£o, simplificando a l√≥gica de consenso.

---

## Vantagens e Desvantagens

| Aspecto                   | Paxos                                             | Raft                                           |
| ------------------------- | ------------------------------------------------- | ---------------------------------------------- |
| **Complexidade**          | Alta, dif√≠cil de entender e implementar           | Projetado para ser mais simples e modular      |
| **Performance**           | Similar, pode ser otimizado                       | Similar, com potencial vantagem pr√°tica        |
| **Toler√¢ncia a falhas**   | Alta, tolera falhas de n√≥s e redes                | Igual, com mecanismo claro de elei√ß√£o de l√≠der |
| **Ado√ß√£o industrial**     | Usado em sistemas como Chubby (Google), Zookeeper | Usado em etcd, Consul, RethinkDB e outros      |
| **Documenta√ß√£o e ensino** | Pouco did√°tico, base acad√™mica rigorosa           | Focado no aprendizado e implementa√ß√£o f√°cil    |

---

Claro! Aqui est√° um texto explicando onde os algoritmos de consenso como **Paxos** e **Raft** s√£o aplicados no dia a dia, e qual o impacto pr√°tico deles no mundo real.

---

# Aplica√ß√µes Pr√°ticas dos Algoritmos de Consenso (Paxos e Raft) e seu Impacto no Dia a Dia

Os algoritmos de consenso distribu√≠do, como Paxos e Raft, s√£o fundamentais para garantir a **confiabilidade**, **disponibilidade** e **consist√™ncia** de sistemas que usamos cotidianamente, mesmo que isso n√£o seja vis√≠vel diretamente para o usu√°rio final.

## Onde s√£o aplicados?

### 1. **Sistemas de Arquivos Distribu√≠dos**

Sistemas de arquivos modernos e distribu√≠dos, como o **Google File System (GFS)** e o **Hadoop Distributed File System (HDFS)**, precisam garantir que v√°rias r√©plicas de dados estejam sempre sincronizadas para evitar perda ou corrup√ß√£o.

O Paxos √© utilizado no **Google Chubby**, um servi√ßo de bloqueio distribu√≠do que coordena o acesso a recursos compartilhados e serve como base para sistemas de arquivos distribu√≠dos.

### 2. **Bancos de Dados Distribu√≠dos**

Bancos de dados modernos, especialmente aqueles que funcionam em clusters distribu√≠dos, precisam garantir que todas as r√©plicas concordem sobre a ordem das opera√ß√µes para manter a integridade dos dados.

Exemplos incluem:

* **etcd**: Um banco de dados chave-valor usado para armazenar configura√ß√µes e coordenar clusters, baseado no Raft.
* **Consul**: Um sistema de descoberta e configura√ß√£o para microservi√ßos que usa Raft para garantir consist√™ncia.
* **Apache Cassandra** e **Google Spanner** utilizam varia√ß√µes do consenso para replica√ß√£o segura.

### 3. **Sistemas de Mensageria e Filas de Mensagens**

Servi√ßos que gerenciam filas e mensagens distribu√≠das, como **Apache Kafka** e **RabbitMQ**, precisam garantir a ordem e a durabilidade das mensagens, mesmo que algumas m√°quinas falhem.

Alguns desses sistemas utilizam protocolos inspirados em Paxos ou Raft para coordenar l√≠deres e replicar logs.

### 4. **Orquestra√ß√£o de Containers e Servi√ßos em Nuvem**

Plataformas como **Kubernetes** usam etcd (baseado em Raft) para armazenar o estado do cluster e garantir que m√∫ltiplos n√≥s estejam sincronizados sobre o estado atual dos servi√ßos e pods, garantindo que o orquestrador tome decis√µes corretas sobre escalabilidade e falhas.

### 5. **Servi√ßos de Alta Disponibilidade**

Muitas aplica√ß√µes que exigem uptime elevado, como sistemas banc√°rios, plataformas de com√©rcio eletr√¥nico, e servi√ßos de telecomunica√ß√µes, dependem desses algoritmos para que um grupo de servidores distribu√≠dos atue como uma √∫nica entidade confi√°vel, mesmo em caso de falhas parciais.

---

## Impacto no Dia a Dia

Embora os usu√°rios finais raramente percebam, os algoritmos de consenso t√™m um impacto direto em sua experi√™ncia:

* **Disponibilidade cont√≠nua:** Ao permitir que sistemas distribu√≠dos continuem operando mesmo com falhas em alguns n√≥s, garantem que servi√ßos essenciais (bancos, redes sociais, plataformas de streaming) estejam sempre dispon√≠veis.
* **Consist√™ncia dos dados:** Evitam situa√ß√µes onde diferentes servidores apresentam dados divergentes, garantindo que uma conta banc√°ria, por exemplo, n√£o fique inconsistente em caso de falhas.
* **Escalabilidade:** Permitem que sistemas cres√ßam horizontalmente (com mais servidores) sem perder a integridade das opera√ß√µes.
* **Recupera√ß√£o r√°pida:** Com mecanismos de elei√ß√£o de l√≠der e replica√ß√£o de logs, esses algoritmos possibilitam a r√°pida recupera√ß√£o e continuidade do servi√ßo ap√≥s falhas.

Os algoritmos de consenso, como Paxos e Raft, s√£o invis√≠veis, mas essenciais para a infraestrutura tecnol√≥gica que sustenta nosso cotidiano digital. Eles garantem que, ao enviar um e-mail, fazer uma transa√ß√£o banc√°ria, usar um aplicativo na nuvem ou assistir a um v√≠deo online, os sistemas distribu√≠dos por tr√°s dessas a√ß√µes funcionem de forma confi√°vel, consistente e resiliente.

Seu impacto vai al√©m da teoria, sendo a base para a confiabilidade de praticamente toda aplica√ß√£o distribu√≠da moderna ‚Äî um pilar invis√≠vel que sustenta a era da computa√ß√£o em nuvem e dos servi√ßos online.

---

Beleza! Vou trazer exemplos pr√°ticos de empresas e servi√ßos que usam **Paxos** e **Raft** no dia a dia, e tamb√©m um pouco mais de detalhes t√©cnicos sobre como esses algoritmos atuam nesses contextos reais.

---

# Exemplos Pr√°ticos e Detalhes T√©cnicos do Uso de Paxos e Raft

## Exemplos Reais de Uso

### 1. **Google ‚Äî Paxos no Chubby**

O **Chubby** √© um servi√ßo de coordena√ß√£o distribu√≠da interno do Google que usa o algoritmo Paxos para fornecer:

* **Servi√ßo de bloqueio distribu√≠do:** Evita que m√∫ltiplos processos acessem simultaneamente recursos compartilhados, como arquivos ou bancos de dados.
* **Armazenamento de configura√ß√µes e elei√ß√£o de l√≠deres:** Serve para que componentes do Google mantenham consist√™ncia de estado e elei√ß√£o de l√≠deres para servi√ßos cr√≠ticos, como o Bigtable.

**Impacto pr√°tico:** Milhares de servi√ßos Google dependem do Chubby para funcionar corretamente, incluindo o Bigtable e o Google File System.

### 2. **CoreOS / Kubernetes ‚Äî Raft no etcd**

O **etcd** √© um banco de dados chave-valor leve usado para armazenar o estado do cluster Kubernetes. Ele implementa o algoritmo Raft para garantir que todas as r√©plicas estejam sincronizadas e que haja um l√≠der eleito para coordenar mudan√ßas.

* **Elei√ß√£o de l√≠der:** O l√≠der Raft aceita requisi√ß√µes de escrita e replica o log para seguidores.
* **Replica√ß√£o de log:** Cada mudan√ßa no estado do cluster √© registrada no log replicado de forma segura.
* **Failover autom√°tico:** Se o l√≠der falhar, um novo √© eleito rapidamente, minimizando downtime.

**Impacto pr√°tico:** O funcionamento confi√°vel do Kubernetes ‚Äî que gerencia milh√µes de containers em produ√ß√£o ‚Äî depende diretamente do consenso garantido por Raft no etcd.

### 3. **HashiCorp Consul**

O **Consul** usa Raft para manter a consist√™ncia do seu banco de dados distribu√≠do que armazena informa√ß√µes de configura√ß√£o e servi√ßos.

* Permite a descoberta din√¢mica de servi√ßos.
* Coordena registros e status com forte consist√™ncia.

---

## Detalhes T√©cnicos do Funcionamento em Contextos Reais

### Paxos em Chubby

* **Pap√©is fixos:** Acceptors, proposers e learners s√£o implementados como servidores e clientes do servi√ßo.
* **Majority quorum:** Para aceitar um valor, √© preciso consenso da maioria dos acceptors.
* **Persist√™ncia:** Os estados dos acceptors s√£o gravados em disco para tolerar falhas.

Essa implementa√ß√£o lida com falhas reais (n√≥s caindo, mensagens perdidas), e √© projetada para oferecer **alta disponibilidade** mesmo em condi√ß√µes adversas.

### Raft em etcd e Consul

* **Elei√ß√£o de l√≠der peri√≥dica:** Cada n√≥ tem um timeout para come√ßar elei√ß√£o se n√£o receber batimentos (heartbeats).
* **Logs replicados:** Cada opera√ß√£o (ex: atualizar configura√ß√£o) √© adicionada ao log do l√≠der e replicada para seguidores, que aplicam as mudan√ßas em ordem.
* **Commit seguro:** Um comando s√≥ √© considerado aplicado ap√≥s confirma√ß√£o da maioria.
* **Estado consistente:** Mesmo em caso de falhas, o protocolo garante que os logs nunca divergem em comandos j√° confirmados.

---

## Por que essa engenharia importa?

Em sistemas distribu√≠dos reais, os desafios s√£o enormes:

* **Falhas e particionamento de rede** podem ocorrer a qualquer momento.
* **Replica√ß√£o e sincroniza√ß√£o de dados** precisam ser r√°pidas para evitar inconsist√™ncia.
* **Escalabilidade e flexibilidade** s√£o necess√°rias para suportar cargas vari√°veis e m√∫ltiplos usu√°rios.

Paxos e Raft fornecem o rigor formal e pr√°tico para lidar com esses desafios, permitindo a constru√ß√£o de sistemas robustos e confi√°veis que usamos todos os dias.

## Implementa√ß√£o simples

* \[1] **Python** ‚Äî orientada a objetos e f√°cil de expandir.
* \[2] **C** ‚Äî focando no n√∫cleo do algoritmo para desempenho e controle.

---

## **Resumo da L√≥gica do Raft Simplificado**

Vamos implementar um sistema com:

* Estados de **Follower**, **Candidate** e **Leader**
* **Elei√ß√£o de l√≠der** com timeout aleat√≥rio
* **Heartbeat** do l√≠der para manter controle

‚ö†Ô∏è *Nota:* Para simplifica√ß√£o, o sistema:

* Usa comunica√ß√£o via chamadas locais (simula√ß√£o de RPC)
* N√£o implementa logs replicados ainda (apenas elei√ß√£o de l√≠der)

---

## [1] Implementa√ß√£o em Python (Raft simplificado)

```python
import random
import threading
import time

FOLLOWER = "Follower"
CANDIDATE = "Candidate"
LEADER = "Leader"

class Node:
    def __init__(self, id, cluster):
        self.id = id
        self.cluster = cluster
        self.state = FOLLOWER
        self.votes = 0
        self.term = 0
        self.leader_id = None
        self.timeout = random.uniform(1.5, 3.0)
        self.reset_timer()

    def reset_timer(self):
        self.timer = threading.Timer(self.timeout, self.start_election)
        self.timer.start()

    def start_election(self):
        self.state = CANDIDATE
        self.term += 1
        self.votes = 1  # vote for self
        print(f"[{self.id}] becomes candidate for term {self.term}")
        for peer in self.cluster:
            if peer.id != self.id:
                peer.request_vote(self)

        self.check_votes()

    def request_vote(self, candidate):
        if candidate.term > self.term:
            self.term = candidate.term
            self.state = FOLLOWER
            candidate.receive_vote()
            print(f"[{self.id}] votes for [{candidate.id}]")
        else:
            print(f"[{self.id}] rejects vote for [{candidate.id}]")

    def receive_vote(self):
        self.votes += 1
        self.check_votes()

    def check_votes(self):
        majority = len(self.cluster) // 2 + 1
        if self.votes >= majority and self.state == CANDIDATE:
            self.become_leader()

    def become_leader(self):
        self.state = LEADER
        self.leader_id = self.id
        print(f"[{self.id}] becomes LEADER for term {self.term}")
        self.timer.cancel()
        self.send_heartbeats()

    def send_heartbeats(self):
        def heartbeat():
            if self.state == LEADER:
                for peer in self.cluster:
                    if peer.id != self.id:
                        print(f"[{self.id}] sends heartbeat to [{peer.id}]")
                threading.Timer(1.0, heartbeat).start()

        heartbeat()

def simulate_cluster(n=3):
    cluster = []
    for i in range(n):
        cluster.append(Node(i, None))
    for node in cluster:
        node.cluster = cluster
    return cluster

if __name__ == "__main__":
    simulate_cluster()
    time.sleep(10)
```

---

## [2] ‚öôÔ∏è Implementa√ß√£o em C (n√∫cleo da elei√ß√£o do l√≠der)

```c
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <unistd.h>

typedef enum {FOLLOWER, CANDIDATE, LEADER} State;

typedef struct Node {
    int id;
    State state;
    int term;
    int votes;
    int total_nodes;
} Node;

void start_election(Node *node, Node *cluster) {
    node->state = CANDIDATE;
    node->term++;
    node->votes = 1;

    printf("[Node %d] is candidate for term %d\n", node->id, node->term);

    for (int i = 0; i < node->total_nodes; i++) {
        if (i == node->id) continue;
        if (node->term > cluster[i].term) {
            cluster[i].term = node->term;
            cluster[i].state = FOLLOWER;
            node->votes++;
            printf("[Node %d] votes for [Node %d]\n", i, node->id);
        }
    }

    if (node->votes > node->total_nodes / 2) {
        node->state = LEADER;
        printf("[Node %d] becomes LEADER\n", node->id);
    }
}

int main() {
    const int N = 3;
    Node cluster[N];

    for (int i = 0; i < N; i++) {
        cluster[i].id = i;
        cluster[i].state = FOLLOWER;
        cluster[i].term = 0;
        cluster[i].votes = 0;
        cluster[i].total_nodes = N;
    }

    srand(time(NULL));
    int random_node = rand() % N;
    sleep(1);
    start_election(&cluster[random_node], cluster);

    return 0;
}
```

---

## üß™ O que voc√™ pode fazer a partir daqui?

* Adicionar **replica√ß√£o de logs**
* Suportar falhas e rein√≠cios
* Usar sockets para simular RPC real
* Visualizar o cluster com logs ou gr√°fico


## Cita√ß√µes de Autores

* Leslie Lamport (1998) define Paxos como "o primeiro protocolo correto para consenso distribu√≠do que √© ass√≠ncrono e tolera falhas de processo".
* Ongaro e Ousterhout (2014) escreveram que "a complexidade do Paxos tem impedido a ampla ado√ß√£o, e Raft foi projetado para ser compreens√≠vel, com uma abordagem pr√°tica para replica√ß√£o de logs".

---

## Conclus√£o

Paxos e Raft s√£o algoritmos essenciais para garantir consenso em sistemas distribu√≠dos com falhas. Paxos trouxe uma base te√≥rica e formal fundamental, mas sua complexidade dificultou sua ado√ß√£o pr√°tica. Raft, por sua vez, conseguiu democratizar o consenso distribu√≠do, facilitando o entendimento, implementa√ß√£o e manuten√ß√£o do protocolo, o que explica sua ampla ado√ß√£o em projetos de c√≥digo aberto e em empresas.

Entender ambos os algoritmos √© crucial para cientistas da computa√ß√£o que trabalham com sistemas distribu√≠dos, bases de dados, e aplica√ß√µes que exigem alta disponibilidade e consist√™ncia.

---

## Refer√™ncias

* Lamport, Leslie. "The Part-Time Parliament." *ACM Transactions on Computer Systems* 16, no. 2 (1998): 133‚Äì169. \[[https://doi.org/10.1145/279227.279229](https://doi.org/10.1145/279227.279229)]
* Ongaro, Diego, and John Ousterhout. "In Search of an Understandable Consensus Algorithm." *USENIX Annual Technical Conference* (2014). \[[https://raft.github.io/raft.pdf](https://raft.github.io/raft.pdf)]
* Fischer, Michael J., Nancy A. Lynch, and Michael S. Paterson. "Impossibility of distributed consensus with one faulty process." *Journal of the ACM (JACM)* 32, no. 2 (1985): 374-382.

